{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nimport pandas as pd\nimport sklearn\n\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n\nfrom sklearn.tree import DecisionTreeRegressor as DTR, DecisionTreeClassifier as DTC\nfrom sklearn.metrics import mean_squared_error as MSE, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n\nfrom catboost import CatBoostClassifier, Pool\nimport catboost\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\n\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# X-regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Необходимо найти наилучшие параметры для XGBRegression, обучить модель и вернуть ее. Данные из [гита](https://github.com/samstikhin/ml2021/tree/master/06-Boosting/data) `Financial Distress.csv`.\n\nСам гридсерч или нативное исследование необходимо делать вне функции обработки, чтобы не получить TL."]}, {"cell_type": "code", "metadata": {}, "source": ["def xreg(X_train, y_train):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return model"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["df = pd.read_csv('data/Financial Distress.csv')\n\nX = df.drop('Financial Distress', axis=1)\ny = df['Financial Distress']\n######################################################\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n\nxgb_model = xreg(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\n\nassert type(xgb_model) == xgb.sklearn.XGBRegressor\nassert MSE(y_pred, y_test) < 3\n######################################################"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# CatFeatures"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Обучите модель классификации катбуста на предложенных данных и верните обученную модель. \n\nВоспользуйтесь встроенной обработкой категориальных признаков. Не забудьте обработать Nan значения.\n\nСкрытых тестов нет, только один датасет `flyight_delays_train.csv` из [гита](https://github.com/samstikhin/ml2021/tree/master/06-Boosting/data)."]}, {"cell_type": "code", "metadata": {}, "source": ["def catfeatures(df: pd.DataFrame):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return model"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["df = pd.read_csv('data/flight_delays_train.csv')\ndf_train = df[:1000]\n\n######################################################\n\nmodel = catfeatures(df_train)\nassert type(model) == catboost.CatBoostClassifier\n\ndf_test = pd.read_csv('data/flight_catfeature_test.csv')\ndf_test = df_test.drop('Unnamed: 0', axis=1)\nX_test = df_test.drop('dep_delayed_15min',axis=1)\ny_test = df_test['dep_delayed_15min']\n\ny_pred = model.predict(X_test)\nassert accuracy_score(y_test, y_pred) > 0.80 \nassert accuracy_score(y_test, y_pred) < 0.87 \n\n######################################################"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# LightGBM"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Вашем вниманию представляется прокаченный градиентный бустинг `LightGBM`. Разобраться в нем вам предлагается самостоятельно, например по [статье на хабре](https://habr.com/ru/company/skillfactory/blog/530594/). \n\nА в задачке, вам необходимо (опять...) найти наилучшие параметры для LGBMRegressor, обучить модель и вернуть ее. Данные из [гита](https://github.com/samstikhin/ml2021/tree/master/06-Boosting/data) `Financial Distress.csv`.\n\nСам гридсерч или нативное исследование необходимо делать вне функции обработки, чтобы не получить TL."]}, {"cell_type": "code", "metadata": {}, "source": ["def lgbmreg(X_train, y_train):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return model"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["df = pd.read_csv('data/Financial Distress.csv')\n######################################################\nX = df.drop('Financial Distress', axis=1)\ny = df['Financial Distress']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n\nlgbm_model = lgbmreg(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\n\nassert type(lgbm_model) == LGBMRegressor\nassert MSE(y_pred, y_test) < 1.2\n######################################################"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Производные для регрессии"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Окей, в лекции было очень много страшных формул, теперь можно осознать зачем это нужно.\n\nПусть мы хотим бустить регрессию со стандартной функцией потерь $MSE$:\n\n$$\\mathcal{L}(a, x,y) = (a(x_i) - y_i)^2$$\n\nНеобходимо найти через взятие производных:\n\n1. Константный вектор $[a_0]_{i=1}^{N}$\n$$a_0(x) = \\arg\\min_{ c\\in \\mathbb{R}} \\sum_{i=1}^n \\mathcal{L}(c, x_i, y_i)$$\n\n2. Градиенты функции потерь\n$$g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(a_t, x_i, y_i)}{\\partial a_t(x_i)}\\Big]_{i=1}^N$$\n\n3. Коэффициенты при композиции \n$$\\eta_{t + 1} = \\arg\\min_\\eta \\sum_{i=1}^N \\mathcal{L}(f_{t} + \\eta b_{t+1}, x_i, y_i)$$\n\n### Sample 1\n#### Input:\n```python\ny = np.array([1, 2, 3])\nf = np.array([2, 2, 2])\nb = np.array([0, 2, 4])\n```\n#### Output:\n```python\nf_0 = 2.0\ng = [-2, 0,  2] \nalpha = 0.2\n\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def init(y_i: np.array) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return a_0\n\ndef grad(a: np.array, y: np.array) -> np.array:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return g\n\ndef eta(f :np.array, b: np.array, y: np.array) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return eta"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["y = np.array([1, 2, 3])\na = np.array([2, 2, 2])\nb = np.array([0, 2, 4])\n\na_0 = init(y)\ng = grad(a, y)\net = eta(a, b, y)\n\nassert np.abs(a_0 - 2.0)   < 1e-9\nassert_array_almost_equal(g, np.array([-2, 0, 2]))\nassert np.abs(et - (0.2)) < 1e-9\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# GradientBoosting"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте градиентный бустинг на решающих деревьях для регрессии с логгированием. Верните модель, которая будет хранить в себе `n_estimators` обученных деревьев и коэффициенты, чтобы с их помощью потом найти результат предсказания.\n\nТакже необходимо реализовать логгирование в течение обучения.\n\n* `self.estimators` - лист c деревьями\n* `self.eta` - лист с коэффициентами eta\n* `self.a_list` - лист со значениями комбинаций алгоритма $a_T(x_i) = a_0(x_i) + \\sum_{t=1}^{T}\\eta_tb_t(x_i)$\n* `self.g_list` - лист с векторами градиентов на каждой итерации $g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(a_t, x_i, y_i)}{\\partial a_t(x_i)}\\Big]_{i=1}^N$\n* `self.b_list` - лист со значениями базового обучаемого дерева на тренировочной выборке на каждой итерации \n\nПримечания:\n\n* Обрывать алгоритм не нужно, необходимо обучить все деревья.\n* Начальный константный вектор из $a_0$ логгировать не нужно, однако не забудьте его добавить в `predict` c нужным количеством объектов!\n\n### Sample 1\n#### Input:\n```python\nn_estimators = 2\nmax_depth=3\nX_train = np.array([[0], [1], [2], [3], [4]])\ny_train = np.array([0, 2, 4, 2, 0])\nX_test  = np.array([[1.2], [2.3]])\ny_test  = np.array([2.2, 3.7])\n```\n#### Output:\n```python\ny_test_pred = [2, 4]\n\nmodel.a_list = [array([0.0, 2.0, 3.0, 3.0, 0.0]),\n                array([0.0, 2.0, 4.0, 2.0, 0.0])]\n\nmodel.g_list = [array([-3.2,  0.8, 4.8, 0.8, -3.2]), \n                array([ 0.0,  0.0, 2.0,-2.0,  0.0])]\n\nmodel.b_list = [array([-3.2, 0.8, 2.8,  2.8, -3.2]), \n                array([ 0.0, 0.0, 2.0, -2.0,  0.0])]\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn.metrics import mean_squared_error\n\nclass MyGradBoost():\n    def __init__(self, n_estimators=10, max_depth=3):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.estimators_ = np.array([DTR(max_depth=self.max_depth) for _ in range(n_estimators)])\n        self.eta = []\n        self.a_list = []\n        self.b_list = []\n        self.g_list = []\n        \n    def fit(self, X_train: np.array, y_train: np.array): \n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n        \n    def predict(self, X_test) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        ### получить результат из массивов a_list, b_list, g_list\n        return y_pred\n    \n    def score(self, X_test, y_test)-> np.array:\n        return mean_squared_error(self.predict(X_test), y_test)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nn_estimators = 2\nmax_depth=3\nX_train = np.array([[0], [1], [2], [3], [4]])\ny_train = np.array([0, 2, 4, 2, 0])\nX_test  = np.array([[1.2], [2.3]])\ny_test  = np.array([2.2, 3.7])\n\nmodel = MyGradBoost(n_estimators=n_estimators, max_depth=max_depth).fit(X_train, y_train)\nassert model.score(X_test, y_test) < 0.2\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# AdaBoost step"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте одну итерацию алгоритма AdaBoost:\n\n1. Обучите дерево $b$ на $X_{train}$ и верните $y_{pred}$ (не забудьте использовать при обучении данные `sample_weights`!)\n\n2. Найдите среднюю взвешенную ошибку:\n$$error = Q(b_t, X, y) = \\sum_{i=1}^{N}w_i^{(t-1)}[y_i \\neq b_t(x)]$$\n\n3. Найдите коэффициент $\\alpha$ (для корректного выполнения добавим $eps$):\n$$\\alpha = \\frac{1}{2}\\ln\\Big(\\frac{1-error + eps}{error + eps}\\Big)$$\n\n4. Найдите новые веса:\n$$w_i^{new} = w_iexp\\Big(-\\alpha y_i b(x_i)\\Big)$$\n$$w_i^{new} = \\frac{w_i^{new}}{\\sum_{i=1}^{N}w_i^{new}}$$\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\ny_train = np.array([-1, -1, -1, 1])\n```\n#### Output:\n```python\ny_pred =  [-1 -1 -1 -1] \nerror = 0.056 \nalpha = 1.417 \nnew_weights = [0.05882403 0.41176819 0.02941201 0.49999576]\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def boost_step(estimator, weights, X_train, y_train, eps = 1e-6):\n    weights /= np.sum(weights) #нормируем веса исходный, если вдруг ненормированы\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return y_pred, error, alpha, new_weights"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\ny_train = np.array([-1, -1, -1, 1])\n\nestimator = DTC(max_depth=1, random_state=4)\nsample_weights = [0.1, 0.7, 0.05, 0.05]\ny_pred, error, alpha, new_weights = boost_step(estimator, sample_weights, X_train, y_train)\nassert_array_almost_equal(y_pred, np.array([-1, -1, -1, -1])) \nassert np.abs(error - 0.056) < 1e-2 \nassert np.abs(alpha - 1.417) < 1e-2 \nassert_array_almost_equal(new_weights, np.array([0.05882403, 0.41176819, 0.02941201, 0.49999576]))\n######################################################\nX_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\ny_train = np.array([-1, -1, 1, 1])\nestimator = DTC(max_depth=1, random_state=6)\nsample_weights = [0.1, 0.7, 0.05, 0.5]\n\ny_pred, error, alpha, new_weights = boost_step(estimator, sample_weights, X_train, y_train)\n\nassert_array_almost_equal(y_pred, np.array([-1, -1, 1, 1])) \nassert np.abs(error - 0.0) < 1e-2 \nassert np.abs(alpha - 6.907) < 1e-2 \nassert_array_almost_equal(new_weights, np.array([0.074074, 0.518519, 0.037037, 0.37037]))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# AdaBoost classifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте AdaBoost для бинарной классификации на деревьях высоты 1. Верните модель, которая будет хранить в себе `n_estimatos` обученных деревьев и коэффициенты, чтобы с их помощью потом найти результат предсказания.\n\nТакже необходимо реализовать логгирование в течение обучения.\n\n* `self.sample_weights_list` - лист с весами объектов на каждой итерации\n* `self.y_pred_list` - лист с предсказанием каждого следующего дерева (не комбинации)\n* `self.error_list` - лист с ошибками\n\nПримечания:\n\n* Обрывать алгоритм не нужно, необходимо обучить все деревья.\n* Начальные веса логгировать не нужно\n* `predict_proba` реализовывать не нужно\n\n### Sample 1\n#### Input:\n```python\nn_estimators = 2\nX_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\ny_train = np.array([-1, -1, -1, 1])\nX_test  = np.array([[1, 0], [5, 5]])\ny_test  = np.array([-1, 1])\n```\n#### Output:\n```python\ny_test_pred = [0, 1]\n\nmodel.sample_weight = [array([0.167, 0.167, 0.167, 0.5]), \n                       array([ 0.1,  0.5,  0.1, 0.3])]\nmodel.y_pred = [array([-1, -1, -1, -1]),\n                array([-1,  1, -1,  1])]\n\nmodel.alpha = [0.25, 0.167]\n```\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nclass MyAdaBoost():\n    def __init__(self, n_estimators=10):\n        self.estimators_ = np.array([DTC(max_depth=1) for _ in range(n_estimators)])\n        self.alpha = []\n        self.sample_weights_list = []\n        self.y_pred_list = []\n        self.error_list = []\n        \n    def fit(self, X_train: np.array, y_train: np.array):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n        \n    def predict(self, X_test) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX_train = np.array([[0, 0], [4, 0], [0, 4], [4, 4]])\ny_train = np.array([-1, -1, -1, 1])\nX_test  = np.array([[1, 0], [5, 5]])\n\nmodel = MyAdaBoost(n_estimators = 2).fit(X_train, y_train)\n\ny_pred_my = model.predict(X_test)\n\nassert_array_almost_equal(y_pred_my, np.array([-1, 1]))\n\n######################################################\nX_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\ny_train = np.array([-1, -1, 1, 1])\nX_test  = np.array([[3, 3], [6, 6]])\n\nmodel = MyAdaBoost(n_estimators = 2).fit(X_train, y_train)\n\ny_pred_my = model.predict(X_test)\n\nassert_array_almost_equal(y_pred_my, np.array([-1, 1]))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Stacking"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Стэкинг** - 3-ий способ комбинирования алгоритмов, кроме бэггинга и бустинга. Он не часто используется, но его идея крайне полезная: `обучение на мета-признаках`.\n\n1. Разобъем нашу обучающую выборку на 2 части: базовую и дополнительную.\n2. Возьмем $N$ базовых алгоритмов и обучим их на **базовой части** разбив на $N$ фолдов. (Разбили на $N$ частей и обучаем алгоритм на всех частях кроме одной, как на кросс-валидации)\n3. Каждым из обученных базовых алгоритмов предскажем значение для **дополнительной** части выборки.\n4. Соберем **мета-выборку**, состоящую из предсказаний базовых алгоритмов на **доп выборе**. Пример: пусть для объекта $x_i$ базовые алгоритмы выдали $(y_i^1 = 1, y_i^2 = 0, y_i^3 = 1)$. Тогда признаками объекта в **мета-выборке** будет вектор $1, 0, 1$.\n5. Обучим **мета-алгоритм** на **мета-выборке**. И получим готовую модель.\n6. Чтобы получить результат на тестовой, теперь нужно сделать предсказания базовыми алгоритмами, собрать **мета-выборку** и сделать предсказания на **мета-алгоритме**.\n\nРеализуйте стекинг классификацию на **деревьях решений**. Валидация проводится на датасете `forest_train.csv` из папки в [гите](https://github.com/samstikhin/ml2021/tree/master/06-Boosting/data)."]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nclass Stacking():\n    def __init__(self, n_estimators=5, max_depth=5):\n        self.max_depth_ = max_depth\n        self.n_estimators_ = n_estimators\n        self.estimators_ = [DTC(max_depth=self.max_depth_) for _ in range(self.n_estimators_)]\n        self.meta_estimator_ = DTC(max_depth=self.max_depth_)\n        \n    def fit(self, X: np.array, y: np.array): \n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n        \n    def predict(self, X_test) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return y_pred"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["df = pd.read_csv('data/forest_train.csv')\n\n######################################################\nX = df.drop(columns=['Cover_Type', 'Id']).reset_index(drop=True)\ny = df['Cover_Type']\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, train_size=0.3)\n\nmodel = Stacking(max_depth=10, n_estimators=3).fit(X_train, y_train)\n\nassert type(model.meta_estimator_) == sklearn.tree.DecisionTreeClassifier\n\ny_pred = model.predict(X_test)\ny_pred1 = model.estimators_[0].predict(X_test)\ny_pred2 = model.estimators_[1].predict(X_test)\ny_pred3 = model.estimators_[2].predict(X_test)\n\nassert accuracy_score(y_pred, y_test) > 0.67\n\nassert accuracy_score(y_pred1, y_test) < accuracy_score(y_pred, y_test)\nassert accuracy_score(y_pred2, y_test) < accuracy_score(y_pred, y_test)\nassert accuracy_score(y_pred3, y_test) < accuracy_score(y_pred, y_test)\n######################################################\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 4}